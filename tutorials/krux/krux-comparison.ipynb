{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a list of 20 comprehensive topics that cover a wide range of aspects of our daily life, the world, and science, ensuring diversity across different domains:\n",
      "\n",
      "1. **Sustainable Living Practices** - Exploring eco-friendly habits for a greener future.\n",
      "2. **Global Economic Trends** - Analyzing shifts in the world economy and their impacts.\n",
      "3. **Mental Health in the Digital Age** - Understanding the intersection of technology and mental wellness.\n",
      "4. **Renewable Energy Innovations** - Delving into the latest in solar, wind, and other renewable energy technologies.\n",
      "5. **Cultural Heritage Preservation** - Efforts to protect historical sites and traditions worldwide.\n",
      "6. **The Future of Space Exploration** - Missions, discoveries, and the quest for life beyond Earth.\n",
      "7. **Nutrition and Dietetics for Wellbeing** - Science-backed dietary advice for optimal health.\n",
      "8. **Cybersecurity Threats and Solutions** - Protecting personal and corporate data in the digital era.\n",
      "9. **Biodiversity and Ecosystem Conservation** - Strategies to preserve natural habitats and species.\n",
      "10. **Artificial Intelligence in Everyday Life** - Applications, benefits, and ethical considerations.\n",
      "11. **Climate Change Mitigation Strategies** - Global and local efforts to reduce carbon footprints.\n",
      "12. **Educational Technology and Learning Outcomes** - How tech is reshaping the classroom experience.\n",
      "13. **The Science of Sleep and Dreams** - Uncovering the mysteries of rest and subconsciousness.\n",
      "14. **Urban Planning for Sustainable Cities** - Designing metropolises of the future with sustainability in mind.\n",
      "15. **Water Conservation and Management** - Techniques for preserving this vital resource.\n",
      "16. **The Ethics of Genetic Engineering** - Debating the moral implications of altering life forms.\n",
      "17. **Travel and Cultural Exchange Programs** - Fostering global understanding through exploration.\n",
      "18. **Disaster Response and Recovery Innovations** - Technological advancements in crisis management.\n",
      "19. **The Impact of Social Media on Society** - Examining the broad effects of social networking.\n",
      "20. **Advancements in Medical Diagnostic Technologies** - How new tech is revolutionizing healthcare diagnostics.\n",
      "\n",
      "These topics are designed to be as diverse as possible, covering aspects of daily life, global issues, scientific advancements, and the intersection of technology with various facets of human existence.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from nemo_curator import OpenAIClient\n",
    "from nemo_curator.synthetic import NemotronGenerator\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=\"NVIDIA_API_KEY\"\n",
    ")\n",
    "client = OpenAIClient(openai_client)\n",
    "generator = NemotronGenerator(client)\n",
    "\n",
    "n_macro_topics = 20\n",
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\"\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "}\n",
    "\n",
    "responses = generator.generate_macro_topics(\n",
    "    n_macro_topics=n_macro_topics, model=model, model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "print(responses[0])\n",
    "# Output:\n",
    "# 1. Climate Change and Sustainable Living\n",
    "# 2. Space Exploration and the Universe\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a limerick about the wonders of GPU computing:\n",
      "\n",
      "There once was a GPU so fine,\n",
      "Whose parallel processing did shine.\n",
      "It crunched with great zest,\n",
      "Through complex tests,\n",
      "And made computations divine.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from nemo_curator import OpenAIClient\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=\"NVIDIA_API_KEY\",\n",
    ")\n",
    "client = OpenAIClient(openai_client)\n",
    "responses = client.query_model(\n",
    "    model=\"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a limerick about the wonders of GPU computing.\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "print(responses[0])\n",
    "# Output:\n",
    "# A GPU with numbers in flight, Brings joy to programmers late at night.\n",
    "# With parallel delight, Solving problems, so bright,\n",
    "# In the realm of computing, it's quite a sight!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Reward Score Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.875\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from nemo_curator import OpenAIClient\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=\"NVIDIA_API_KEY\",\n",
    ")\n",
    "client = OpenAIClient(openai_client)\n",
    "\n",
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-reward\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Ah, Paris, the City of Light! There are so many amazing things to see and do in this beautiful city ...\",\n",
    "    },\n",
    "]\n",
    "\n",
    "rewards = client.query_reward_model(messages=messages, model=model)\n",
    "print(rewards)\n",
    "# {\n",
    "# \"helpfulness\": 1.6171875\n",
    "# \"correctness\": 1.6484375\n",
    "# \"coherence\": 3.3125\n",
    "# \"complexity\": 0.546875\n",
    "# \"verbosity\": 0.515625\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the importance of including a module d...</td>\n",
       "      <td>[' appropriate best practice file naming conve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://lilianweng.github.io/post...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the response generation stage in Hugg...</td>\n",
       "      <td>['gingFace platform according to the model des...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://lilianweng.github.io/post...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some common limitations faced when de...</td>\n",
       "      <td>[' code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\...</td>\n",
       "      <td>Some common limitations faced when developing ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://lilianweng.github.io/post...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does Algorithm Distillation (AD) improve t...</td>\n",
       "      <td>[' encapsulated in a long history-conditioned ...</td>\n",
       "      <td>Algorithm Distillation (AD) improves the learn...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://lilianweng.github.io/post...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key components and functionalitie...</td>\n",
       "      <td>['\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | ...</td>\n",
       "      <td>The key components and functionalities of LLM-...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://lilianweng.github.io/post...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the importance of including a module d...   \n",
       "1  How does the response generation stage in Hugg...   \n",
       "2  What are some common limitations faced when de...   \n",
       "3  How does Algorithm Distillation (AD) improve t...   \n",
       "4  What are the key components and functionalitie...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [' appropriate best practice file naming conve...   \n",
       "1  ['gingFace platform according to the model des...   \n",
       "2  [' code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\...   \n",
       "3  [' encapsulated in a long history-conditioned ...   \n",
       "4  ['\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | ...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0                                                NaN         simple   \n",
       "1                                                NaN         simple   \n",
       "2  Some common limitations faced when developing ...         simple   \n",
       "3  Algorithm Distillation (AD) improves the learn...         simple   \n",
       "4  The key components and functionalities of LLM-...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'https://lilianweng.github.io/post...          True  \n",
       "1  [{'source': 'https://lilianweng.github.io/post...          True  \n",
       "2  [{'source': 'https://lilianweng.github.io/post...          True  \n",
       "3  [{'source': 'https://lilianweng.github.io/post...          True  \n",
       "4  [{'source': 'https://lilianweng.github.io/post...          True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ragas = pd.read_csv('ragas_dataset.csv')\n",
    "ragas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NeMo Questions using Ragas Contexts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'contexts', 'ground_truth', 'evolution_type', 'metadata',\n",
       "       'episode_done', 'sdg_nemotron_q', 'sdg_nemotron_a',\n",
       "       'sdg_nemotron_reward', 'sdg_nemotron_ragas_a'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the significance of adhering to a language-specific best practice for file naming conventions when organizing multiple classes and functions across different files in a project?\n",
      "How does the task execution stage in HuggingGPT address challenges related to model inference latency, expert model selection accuracy, and result interpretation complexity?\n",
      "What are the primary architectural trade-offs when designing LLM-powered autonomous agent systems to mitigate limitations in context length, long-term planning, and natural language interface reliability?\n",
      "How does Algorithm Distillation (AD) leverage multi-episode learning histories to enhance in-context reinforcement learning, compared to baselines like Expert Distillation (ED) and RL^2?\n",
      "What are the core architectural elements and operational workflows of LLM-powered autonomous agents that enable effective planning, memory utilization, and tool integration?\n",
      "How do consistent naming conventions, modular imports, and dependency definition files (e.g., requirements.txt for Python, package.json for NodeJS) collectively enhance code compatibility across different files and projects?\n",
      "How does API-Bank assess LLMs' decision-making across multiple API tools in iterative task refinement?\n",
      "How does a package.json fit into defining a NodeJS project's core components?\n",
      "How does API-Bank assess LLMs' utilization of ANNOY/HNSW for efficient API search and selection, impacting overall tool-augmented performance?\n",
      "How do finite context lengths and long-term planning challenges impact the overall robustness of LLM-powered autonomous agents?\n"
     ]
    }
   ],
   "source": [
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\"\n",
    "\n",
    "question_lst = []\n",
    "\n",
    "for i in range(len(ragas)):\n",
    "    closed_qa_responses = client.query_model(\n",
    "        model=model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate a single, concise question similar to '{ragas.question[i]}' based on the provided context. Do NOT include any explanations, rationale, or additional text in your response—just the question itself: {ragas.contexts[i]}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.7,\n",
    "        max_tokens = 1024,\n",
    "    )\n",
    "    \n",
    "    question = closed_qa_responses[0]\n",
    "    question_lst.append(question)\n",
    "    \n",
    "ragas['sdg_nemotron_q'] = question_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NeMo Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\"\n",
    "\n",
    "answer_lst = []\n",
    "\n",
    "for i in range(len(ragas)):\n",
    "    responses = client.query_model(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate a single, concise answer to '{ragas.sdg_nemotron_q[i]}' based on the provided context. Do NOT include any explanations, rationale, or additional text or special character patterns in your response—just the answer itself: {ragas.contexts[i]}\"\n",
    "             }\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    answer = responses[0]\n",
    "    answer_lst.append(answer)\n",
    "\n",
    "ragas['sdg_nemotron_a'] = answer_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 .\n",
      "What is the significance of adhering to a language-specific best practice for file naming conventions when organizing multiple classes and functions across different files in a project?\n",
      "Ensures maintainability, compatibility, readability, and scalability by providing a consistent structure, facilitating collaboration, and enabling efficient dependency management across the project.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1 .\n",
      "How does the task execution stage in HuggingGPT address challenges related to model inference latency, expert model selection accuracy, and result interpretation complexity?\n",
      "**Task Execution Stage in HuggingGPT Addresses Challenges as Follows:**\n",
      "\n",
      "* **Model Inference Latency:** Expert models execute specific tasks in parallel, reducing overall latency.\n",
      "* **Expert Model Selection Accuracy:** LLM frames task as a multiple-choice question, ensuring accurate model selection from a filtered list based on task type.\n",
      "* **Result Interpretation Complexity:** AI assistant describes the process and results in a straightforward, first-person manner, providing analysis and model inference results, including complete file paths if applicable.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "2 .\n",
      "What are the primary architectural trade-offs when designing LLM-powered autonomous agent systems to mitigate limitations in context length, long-term planning, and natural language interface reliability?\n",
      "**Primary Architectural Trade-offs for LLM-Powered Autonomous Agent Systems:**\n",
      "\n",
      "1. **Context Window Expansion**:\n",
      "\t* **Trade-off**: Increased computational resources vs. enhanced historical context and self-reflection capabilities.\n",
      "\t* **Mitigation Strategies**: Vector stores, retrieval mechanisms, or hierarchical attention models.\n",
      "2. **Long-term Planning and Task Decomposition**:\n",
      "\t* **Trade-off**: Complexity of planning algorithms vs. robustness to unexpected errors and exploration efficiency.\n",
      "\t* **Mitigation Strategies**: Integrating discrete reasoning, modular architectures (e.g., MRKL Systems), or algorithm distillation techniques.\n",
      "3. **Natural Language Interface Reliability**:\n",
      "\t* **Trade-off**: Output parsing complexity vs. reliability and adherence to instructions.\n",
      "\t* **Mitigation Strategies**: Implementing deliberate problem-solving frameworks (e.g., Tree of Thoughts), feedback alignment (e.g., Chain of Hindsight), or synergizing reasoning and acting (e.g., ReAct).\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "3 .\n",
      "How does Algorithm Distillation (AD) leverage multi-episode learning histories to enhance in-context reinforcement learning, compared to baselines like Expert Distillation (ED) and RL^2?\n",
      "Algorithm Distillation (AD) leverages multi-episode learning histories to enhance in-context reinforcement learning by concatenating and feeding the history into a model, allowing it to learn the RL process itself, whereas baselines like Expert Distillation (ED) rely on expert trajectories and RL^2 requires online RL, with AD outperforming ED and approaching RL^2's performance using only offline RL.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "4 .\n",
      "What are the core architectural elements and operational workflows of LLM-powered autonomous agents that enable effective planning, memory utilization, and tool integration?\n",
      "**Core Architectural Elements and Operational Workflows of LLM-Powered Autonomous Agents:**\n",
      "\n",
      "* **Planning:**\n",
      "\t1. **Subgoal Decomposition:** Break down large tasks into smaller, manageable subgoals (via Chain of Thought, Tree of Thoughts, simple prompting, task-specific instructions, or human inputs)\n",
      "\t2. **Self-Reflection and Refinement:** Self-criticism and learning from past actions to improve future steps\n",
      "* **Memory Utilization:**\n",
      "\t1. **Short-Term Memory:** In-context learning (Prompt Engineering) for task-specific information\n",
      "\t2. **Long-Term Memory:** External vector store for retaining and recalling information over extended periods\n",
      "* **Tool Integration:**\n",
      "\t1. **API Calls:** Leveraging external APIs for additional information, code execution, proprietary sources, and more\n",
      "\t2. **Optional External Tools:** Classical planners (e.g., LLM+P approach with PDDL for long-horizon planning)\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "5 .\n",
      "How do consistent naming conventions, modular imports, and dependency definition files (e.g., requirements.txt for Python, package.json for NodeJS) collectively enhance code compatibility across different files and projects?\n",
      "Collectively, consistent naming conventions, modular imports, and dependency definition files enhance code compatibility across different files and projects by:\n",
      "* Establishing a unified structure and readability (naming conventions)\n",
      "* Encapsulating and reusing code efficiently while minimizing conflicts (modular imports)\n",
      "* Ensuring version consistency and reproducibility of project environments (dependency definition files)\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "6 .\n",
      "How does API-Bank assess LLMs' decision-making across multiple API tools in iterative task refinement?\n",
      "API-Bank assesses LLMs' decision-making across multiple API tools in iterative task refinement through evaluating three key decision levels: \n",
      "1. **API Call Necessity**: Accuracy in determining whether an API call is needed.\n",
      "2. **API Identification and Refinement**: Effectiveness in identifying the right API to call and iteratively modifying API inputs (e.g., search keywords) if initial choices are not satisfactory.\n",
      "3. **Response and Iterative Refinement**: Ability to interpret API results, decide whether to refine and recall if results are unsatisfactory, and generate appropriate responses based on the outcomes.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "7 .\n",
      "How does a package.json fit into defining a NodeJS project's core components?\n",
      "A `package.json` file fits into defining a NodeJS project's core components by serving as the **module dependency or package manager dependency definition file**, outlining project metadata, dependencies, scripts, and versions, thereby providing a single source of truth for the project's structure and dependencies.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "8 .\n",
      "How does API-Bank assess LLMs' utilization of ANNOY/HNSW for efficient API search and selection, impacting overall tool-augmented performance?\n",
      "API-Bank assesses LLMs' utilization of ANNOY/HNSW for efficient API search and selection by evaluating the accuracy of LLMs' decisions at three levels: \n",
      "1. **API Call Necessity**: Whether an API call is needed.\n",
      "2. **API Identification**: Identifying the right API to call, potentially involving iterative modifications (e.g., refining search keywords for Search Engine API).\n",
      "3. **Response Refinement**: Responding based on API results, including the option to refine and recall if initial results are unsatisfactory, indirectly measuring the efficiency of ANNOY/HNSW in facilitating these processes.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "9 .\n",
      "How do finite context lengths and long-term planning challenges impact the overall robustness of LLM-powered autonomous agents?\n",
      "Finite context lengths and long-term planning challenges significantly erode the robustness of LLM-powered autonomous agents by limiting historical information retention, hindering effective task decomposition, and complicating adaptation to unexpected errors, thus undermining overall reliability and adaptability.\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, '.')\n",
    "    print(question_lst[i])\n",
    "    print(answer_lst[i])\n",
    "    print('-----------------------------------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Reward Score for NeMo Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_lst = []\n",
    "\n",
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-reward\"\n",
    "\n",
    "for i in range(len(ragas)):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                            Provide a concise and well-reasoned answer to the following question:\n",
    "                            \"{ragas.sdg_nemotron_q[i]}\"\n",
    "                            Use the context below to ensure accuracy and clarity:\n",
    "                            {ragas.contexts[i]}\n",
    "                        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": ragas.sdg_nemotron_a[i],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    \n",
    "    rewards = client.query_reward_model(messages=messages, model=model)\n",
    "    rewards = float(rewards)\n",
    "    \n",
    "    reward_lst.append(rewards)\n",
    "    \n",
    "ragas['sdg_nemotron_reward'] = reward_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context limits and task challenges affect LLM agent reliability by restricting the inclusion of historical information and detailed instructions due to finite context length. This limitation impacts long-term planning and task decomposition, making it difficult for LLMs to adjust plans when faced with unexpected errors. Additionally, the reliance on natural language interfaces can lead to formatting errors and unreliable outputs, further affecting the reliability of LLM agents.\n"
     ]
    }
   ],
   "source": [
    "print(ragas.ground_truth[i])\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NeMo Answers for Ragas Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\"\n",
    "\n",
    "nemo_answer_lst = []\n",
    "\n",
    "for i in range(len(ragas)):\n",
    "    responses = client.query_model(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate a single, concise answer to '{ragas.question[i]}' based on the provided context. Do NOT include any explanations, rationale, or additional text or special character patterns in your response—just the answer itself: {ragas.contexts[i]}\"\n",
    "             }\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    response = responses[0]\n",
    "    nemo_answer_lst.append(response)\n",
    "\n",
    "ragas['sdg_nemotron_ragas_a'] = nemo_answer_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key components and functionalities of LLM-powered autonomous agents include: 1) Planning: This involves task decomposition, where large tasks are broken down into smaller, manageable subgoals, and self-reflection, where the agent learns from past actions to improve future results. 2) Memory: This includes short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods. 3) Tool Use: The agent learns to call external APIs for additional information, code execution, and access to proprietary information sources.\n",
      "----------------------------------\n",
      "**Key Components and Functionalities of LLM-Powered Autonomous Agents:**\n",
      "\n",
      "1. **Planning**\n",
      "\t* Task Decomposition (subgoal identification, step-by-step breakdown)\n",
      "\t* Self-Reflection (self-criticism, refinement of past actions for improvement)\n",
      "2. **Memory**\n",
      "\t* Short-term Memory (in-context learning, prompt engineering)\n",
      "\t* Long-term Memory (external vector store for retaining and recalling information)\n",
      "3. **Tool Use**\n",
      "\t* Leveraging external APIs for additional information (current data, code execution, proprietary sources)\n"
     ]
    }
   ],
   "source": [
    "print(ragas[ragas.ground_truth != '']['ground_truth'][4])\n",
    "print('----------------------------------')\n",
    "print(ragas[ragas.ground_truth != '']['sdg_nemotron_ragas_a'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the reuslts to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas.to_csv('ragas_nemotron_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
